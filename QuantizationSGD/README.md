## QuantizationSGD

- `signSGD`
    1. **《signSGD: compressed optimisation for non-convex problems》**
    2. ICML 2018：https://icml.cc/Conferences/2018/Schedule?showEvent=2616
    3. ArXiv：https://arxiv.org/abs/1802.04434
    4. Code：https://github.com/jxbz/signSGD

来自瑞士的学者：  
- `Error Feedback Fixes SignSGD`
    1. **《Error Feedback Fixes SignSGD and other Gradient Compression Schemes》**
    2. ICML 2019：https://icml.cc/Conferences/2019/Schedule?showEvent=4158
    3. ArXiv：https://arxiv.org/abs/1901.09847
    4. Code：https://github.com/epfml/error-feedback-SGD
- `Sparsified SGD with Memory`
    1. **《Sparsified SGD with Memory》**
    2. NeurIPS 2018：https://papers.nips.cc/paper/7697-sparsified-sgd-with-memory
    3. ArXiv：https://arxiv.org/abs/1809.07599
    4. Code：https://github.com/epfml/sparsifiedSGD

南大 LAMDA ：  
- `Quantized Epoch-SGD`
    1. **《Quantized Epoch-SGD for Communication-Efficient Distributed Learning》**
    2. 论文一作主页：http://lamda.nju.edu.cn/zhaosy/
    3. ArXiv：https://arxiv.org/abs/1901.03040

- `high-accuracy low-precision training`
    1. **《High-Accuracy Low-Precision Training》**
    2. ArXiv：https://arxiv.org/abs/1803.03383
    3. 论文最新版本：https://www.cs.cornell.edu/~cdesa/papers/arxiv2018_lpsvrg.pdf
    4. Code：https://github.com/HazyResearch/halp[对应论文最新版本]