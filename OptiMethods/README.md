## OptiMethods

- `SparsifiedOptimization.md`
    1. 稀疏优化 and 组稀疏优化
- `SWALP.md` — https://arxiv.org/abs/1904.11943
    1. **《SWALP : Stochastic Weight Averaging in Low-Precision Training》**
- `Universal Mirror-Prox.md` — https://arxiv.org/abs/1902.01637
    1. **《A Universal Algorithm for Variational Inequalities Adaptive to Smoothness and Noise》**


- 《Accelerating Stochastic Gradient Descent using Predictive Variance Reduction》—— ZhangTong
- 《Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients》

**Sign**
- **《signSGD: Compressed Optimisation for Non-Convex Problems》**
    1. signSGD 收敛性分析，以及 majority vote
    2. ArXiv：https://arxiv.org/pdf/1802.04434
- **《signSGD via Zeroth-Order Oracle》**(ICLR 2019)
    1. signSGD 和 Zeroth-Order 的结合
    2. OpenReview：https://openreview.net/forum?id=BJe-DsC5Fm


**Zeroth-Order**
- 《Zeroth-Order Online Alternating Direction Method of Multipliers: Convergence Analysis and Applications》
- 《Zeroth-Order Optimization and Its Application to Adversarial Machine Learning》
- 《Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization》
- 《Zeroth-Order Stochastic Projected Gradient Descent for Nonconvex Optimization》


