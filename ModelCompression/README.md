## ModelCompression

- `Survey`
    1. **《A Survey of Model Compression and Acceleration for Deep Neural Networks》**
    2. ArXiv：https://arxiv.org/abs/1710.09282
- `Deep Compression`
    1. **《Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding》**
    2. ArXiv：https://arxiv.org/abs/1510.00149
    3. ICLR 2016 [Best Paper]：https://iclr.cc/archive/www/doku.php%3Fid=iclr2016:main.html
    4. Code：https://github.com/songhan/Deep-Compression-AlexNet

- `Binarized Neural Networks`
    1. **《Binarized Neural Networks》**
    2. NeurIPS 2016：http://papers.nips.cc/paper/6573-binarized-neural-networks
    3. **《Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1》**
    4. ArXiv：https://arxiv.org/abs/1602.02830
    5. **《BinaryConnect: Training Deep Neural Networks with binary weights during propagations》**
    6. NeurIPS 2015：http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations
- `Distilling the Knowledge`
    1. **《Distilling the Knowledge in a Neural Network》**
    2. ArXiv：https://arxiv.org/abs/1503.02531

- `MobileNets`
    1. **《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications》**
    2. ArXiv：http://arxiv.org/abs/1704.04861
- `ShuffleNet`
    1. **《ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices》**
    2. ArXiv：http://arxiv.org/abs/1707.01083
- `SqueezeNet`
    1. **《SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size》**
    2. ArXiv：http://arxiv.org/abs/1602.07360
- `XNOR-Net`
    1. **《XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks》**
    2. ArXiv：http://arxiv.org/abs/1603.05279

- `AMC`
    1. **《AMC: AutoML for Model Compression and Acceleration on Mobile Devices》**
    2. ArXiv：http://arxiv.org/abs/1802.03494



侯璐（香港科学技术大学）
- 个人主页：http://www.cse.ust.hk/~lhouab/
- `Loss-Aware Binarization`
    1. [Proceedings of the Fifth ICLR 2017 ]**《Loss-aware Binarization of Deep Networks》**
    2. Code：https://github.com/houlu369/Loss-aware-Binarization
- `Loss-Aware Weight Quantization`
    1. [Proceedings of the Sixth ICLR 2018]**《Loss-aware Weight Quantization of Deep Networks》**
    2. Code：https://github.com/houlu369/Loss-aware-weight-quantization


相关博客：
- 《五种CNN模型的尺寸，计算量和参数数量对比详解》
    1. http://m.elecfans.com/article/598165.html
- 《Channel Pruning for Accelerating Very Deep Neural Networks》
    1. Yihui He (何宜晖)：https://github.com/yihui-he/channel-pruning
    2. 网友博客：https://blog.csdn.net/u014380165/article/details/79811779
- 《AMC: AutoML for Model Compression and Acceleration on Mobile Devices》 Yihui He
    1. http://www.sohu.com/a/253814520_129720
    2. https://www.itcodemonkey.com/article/9140.html
    3. http://www.dataguru.cn/article-14195-1.html
    4. https://cloud.tencent.com/developer/news/321681
    5. 让算法解放算法工程师----NAS综述 - 陈泰红的文章 - 知乎 https://zhuanlan.zhihu.com/p/44576620
